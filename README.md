# MathFoundationRL_note

## 西湖大学 强化学习笔记
### 第五课

---

### 1. MC basic

#### 问题1关于MC basic和截断的policy iteration有什么区别？

**答案1：** 简而言之，截断的PL它知道世界的运行规律，即state的转换概率以及行动action获得的奖励概率，它通过这些P以及pi最终可以迭代遍历出state value，再通过这个精准的state来进行max_action的优化；而 MC 则缺少这些P，而只能用一次次遍历最终得到平均的action value。

---

### 2. MC exploring start

#### 问题1MC basic的整体实现，pi不变？
**答案1：**： Basic MC 的单次采样过程中，pi是不变的，后面的inprovement才会变

#### 问题2pi和model free的区别
**答案2：**：pi是策略，在什么state进行什么action的选择问题，而model free是进行action后state的概率分布以及执行action后的reward概率分布

#### 问题3MC exploring start和MC basic区别？
**答案3：** basic会使用两个for循环来遍历所有的(s,a),最终算出一个exp，然后再更新pi。而前者会通过遍历一个（s，a）通过在一个遍历序列中提取多个return，可以高效利用数据，并且是一个episode就更新一次pi。


### 3. MC epsilon-greedy
无问题，只是引入了一个epsilon因子，在policy improvement中选择action并不是选择最大的action state，而是使用概率，其他非最大值的动作也可能被选取到（会引出一个问题，当惩罚很重时候，也会有一定概率走向悬崖，引入后面的sora和q learning。
“引入概率是如何把 State Value（状态价值）拉低”的过程，本质上就是：好动作的权重被稀释了，坏结果的惩罚被“强制”加权进来了。
智能体“趋利避害”的本质，就是数学上的“追求更高的期望回报（State Value）”。为了帮你把这个逻辑彻底闭环，我们可以对比一下**“在悬崖边”和“在中间”**发生随机动作时的数学代价，你立刻就能明白为什么它“不得不”选中间。假设 $\epsilon = 0.1$（10% 概率乱动），掉进悬崖罚 -100，走普通一步罚 -1。1. 场景一：站在悬崖边 (Risky Area)如果不乱动： 往右走一步就到终点（假设得 +10 分）。如果乱动 (10%概率)： 可能会向下乱冲。后果： 只要有一次掉下去，就是 -100。数学期望： 这一个 -100 的巨大负值，会被加权进当前的 Value 里。$$Value \approx 0.9 \times 10 + 0.1 \times (-100) = \mathbf{-1}$$(即使 90% 的时候都很顺，但那 10% 的暴雷把分数彻底拉垮了)2. 场景二：站在路中间 (Safe Area)如果不乱动： 往右走一步（虽然离终点远，得 -1 分）。如果乱动 (10%概率)： 可能会向下乱冲。后果： 向下乱冲是跳到了另一块普通地板上（仅仅是多走了一步冤枉路，再罚 -1）。数学期望：$$Value \approx 0.9 \times (-1) + 0.1 \times (-1) = \mathbf{-1}$$(这里虽然没有奖励，但也没有暴雷)关键点：虽然上面两个简化的计算看起来差不多，但在真实的迭代中，“悬崖边”的状态价值会因为邻居是悬崖而被无限拉低，而“路中间”的状态价值只是稍微因为多走几步路而降低一点点。对比结果：悬崖边的 Value 可能收敛到 -50。路中间的 Value 可能收敛到 -15。智能体决策： $\arg\max(-50, -15)$ -> 选 -15（走中间）！
